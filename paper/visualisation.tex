%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2019}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2019}



\usepackage{filecontents}

\begin{filecontents*}{bibliography.bib}
@article{simonyan2013deep,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}
@article{springenberg2014striving,
  title={Striving for simplicity: The all convolutional net},
  author={Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1412.6806},
  year={2014}
}
@article{selvaraju2016grad,
  title={Grad-CAM: Why did you say that?},
  author={Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
  journal={arXiv preprint arXiv:1611.07450},
  year={2016}
}
@article{smilkov2017smoothgrad,
  title={Smoothgrad: removing noise by adding noise},
  author={Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:1706.03825},
  year={2017}
}
@inproceedings{adebayo2018sanity,
  title={Sanity checks for saliency maps},
  author={Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9505--9515},
  year={2018}
}
@article{wachter2017counterfactual,
  title={Counterfactual explanations without opening the black box: Automated decisions and the GDPR},
  author={Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={841},
  year={2017},
  publisher={HeinOnline}
}
@article{goyal2019counterfactual,
  title={Counterfactual visual explanations},
  author={Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={arXiv preprint arXiv:1904.07451},
  year={2019}
}
@article{singla2019explanation,
  title={Explanation by Progressive Exaggeration},
  author={Singla, Sumedha and Pollack, Brian and Chen, Junxiang and Batmanghelich, Kayhan},
  journal={arXiv preprint arXiv:1911.00483},
  year={2019}
}
@article{zhou2014object,
  title={Object detectors emerge in deep scene cnns},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  journal={arXiv preprint arXiv:1412.6856},
  year={2014}
}
@inproceedings{fong2017interpretable,
  title={Interpretable explanations of black boxes by meaningful perturbation},
  author={Fong, Ruth C and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3429--3437},
  year={2017}
}
@article{singla2019explanation,
  title={Explanation by Progressive Exaggeration},
  author={Singla, Sumedha and Pollack, Brian and Chen, Junxiang and Batmanghelich, Kayhan},
  journal={arXiv preprint arXiv:1911.00483},
  year={2019}
}
@article{joshi2018xgems,
  title={xGEMs: Generating examplars to explain black-box models},
  author={Joshi, Shalmali and Koyejo, Oluwasanmi and Kim, Been and Ghosh, Joydeep},
  journal={arXiv preprint arXiv:1806.08867},
  year={2018}
}
@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4401--4410},
  year={2019}
}
@article{brock2018large,
  title={Large scale gan training for high fidelity natural image synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  journal={arXiv preprint arXiv:1809.11096},
  year={2018}
}
@article{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017}
}
@inproceedings{buolamwini2018gender,
  title={Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author={Buolamwini, Joy and Gebru, Timnit},
  booktitle={Conference on fairness, accountability and transparency},
  pages={77--91},
  year={2018}
}
@inproceedings{samangouei2018explaingan,
  title={ExplainGAN: Model Explanation via Decision Boundary Crossing Transformations},
  author={Samangouei, Pouya and Saeedi, Ardavan and Nakagawa, Liam and Silberman, Nathan},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={666--681},
  year={2018}
}
@article{liu2019generative,
  title={Generative counterfactual introspection for explainable deep learning},
  author={Liu, Shusen and Kailkhura, Bhavya and Loveland, Donald and Han, Yong},
  journal={arXiv preprint arXiv:1907.03077},
  year={2019}
}
\end{filecontents*}


\begin{document}

\twocolumn[
\icmltitle{Classifier-Agnostic Crossing of Decision Boundary under the Image Manifold}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}
\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\begin{abstract}
 
\end{abstract}

\section{Introduction}

% why interpretability/explainability is important?
Understanding the behaviour of complex machine learning predictors is important for debugging and for improving their performance and robustness. However, performance alone is not enough -- even if a classifier has 100\% accuracy for a given class, it could still fail to understand that particular class or concept. For example, if all images of a polar bear exhibit a white background, the classifier can learn to make the decision based on the background pixels. Therefore, methods that \emph{interpret} what the classifier learned can help flag hidden bises in the data \cite{buolamwini2018gender}, evaluate the fairness of the model \cite{doshi2017towards}, and build trust in the system [XX]. 

% Types of visualisers/interpreters
Saliency methods \cite{simonyan2013deep,springenberg2014striving,selvaraju2016grad,smilkov2017smoothgrad} are popular tools for interpreting predictors by highlighting the most important features in an image yielding the given classification. While saliency methods have proven useful for a variety of applications [XX], some of them were shown to be agnostic to the predictor model and hence cannot help with debugging \cite{adebayo2018sanity}. 

Non-salient methods of interpretability have also been developed, that rely on local occlusions or perturbations to the input \cite{zhou2014object,fong2017interpretable}. Yet another class of methods have focused on counterfactual explanations \cite{wachter2017counterfactual,goyal2019counterfactual}, tring to show in what way does the input need to change in order to be assigned a different class. However, one disadvantage of these methods is that such perturbations are not guaranteed to generate realistic looking images, which diminushes their usefulness for end users (e.g. clinicians) \cite{singla2019explanation}.

Recently, some methods \cite{singla2019explanation,samangouei2018explaingan,liu2019generative} have used generative adversarial netoworks (GANs) in order to explore the image manifold and use it to interpret black-box classifier results. However, the design is sub-optimal as it requires re-training the GAN every time the black-box classifier is modified, which is something that can happen often, especially when debugging through progressive refinement. This is especially difficult for state-of-the-art \cite{karras2019style,brock2018large} generators that are complex to train and require large ammounts of computational power. While \cite{joshi2018xgems} employ variational-autoencoders which are easier to train, they do not yield realistic looking images.

We propose a method for visualising a black-box classifier by generating realistic images that cross the classifier's decision boundary. More precisely, we assume a fixed, pre-trained generator $G$ that can generate realistic images. This makes our work applicable not only for uncovering biases in the classifier, but also for debugging and improving its performance through progressive refinement. Since our method does not require training the generator, we can also use pre-trained off-the-shelf generators available online. 

\emph{\textbf{I'm thinking}: Probably we should add active learning as an objective as well. The idea is that once we identify a generated image that is wrongly classified, we can add it along with the correct label to the training dataset. This augmented dataset would hopefully improve performance. If we do this, we can then build a system that generates many paths from ``promising'' starting points, and augment the dataset with manually labeled images that are close to the boundary.}


\newcommand{\bz}{\textbf{z}}

\section{Related work}

% Perturbation-based methods:
% 
% Saliency map-based methods:
% 
% Generative explanation-based methods:

\section{Method}


We assume a black box classifier $f: X \to R$, an initial input image $x \in X$, and a target class $C$. Without loss of generality, we assume the classifier is binary and that $f(x) \in [0,1]$ is the probability of assigning $x$ to target class $C$. Moreover, compared to \cite{singla2019explanation}, we also assume a fixed, pre-trained generator $G: Z \to X$, generating realistic images. We aim to find a trajectory of $T$ synthetic images $\textbf{x} = (x_1, ..., x_T)$ corresponding to latent points $\bz = (z_1, ..., z_T)$, $z_i \in Z$ such that:
\begin{itemize}
 \item \textbf{Monotonicity}: the trajectory monotonically converges towards the target class $C$: $f(z_t+1) > f(z_{t})$
 \item \textbf{Identity preservation}: latent points $z_t$ are as close as possible to the embedding of the source image $G^{-1}(x)$. For example in computer vision or medical imaging, this helps preserve the identity of the person studied, ensuring only minimal changes are made in order to change the label of the point to target class $C$.
\end{itemize}


\section{Option 1: Gradient descent along the classifier softmax}

Without loss of generality, assume $f(x) = 0$. Start with initial point $z_0 = G^{-1}(x)$ and generate points $z_{t+1}$ as follows:

$z_{t+1} = z_t + \eta \nabla f(G(z_t))$


\section{Option 2: Reguarised optimisation}


We define a ``step'' loss on classifier $f$, ensuring that the target value corresponding to $z_{t}$ is $\frac{t}{T}$: 
\begin{equation}
L_f(\bz) = \sum_{t=1}^{T-1} |f(G(z_{t+1})) - \frac{t}{T}| 
\end{equation}

The above loss ensures monotonicity in the assignment of $f$ to class $C$ as we progress along the image manifold. We further define an identity preservation loss, e.g. to ensure that the anatomy of the subject provided in image $x$ is preserved:
\begin{equation}
L_{id}(\bz) = \sum_{t=1}^{T-1} |G^{-1}(x) - G(z_t)| 
\end{equation}

The overall loss is a weighted sum of each loss component:
\begin{equation}
L(\bz, \lambda) = L_f(\bz) + \lambda L_{id}(\bz) 
\end{equation}

Parameters $\bz, \lambda$ need to be optimised.

\section{Results}

\section{Discussion}


\bibliography{bibliography}
\bibliographystyle{icml2019}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \appendix
% \section{Do \emph{not} have an appendix here}
% 
% \textbf{\emph{Do not put content after the references.}}
% %
% Put anything that you might normally include after the references in a separate
% supplementary file.
% 
% We recommend that you build supplementary material in a separate document.
% If you must create one PDF and cut it up, please be careful to use a tool that
% doesn't alter the margins, and that doesn't aggressively rewrite the PDF file.
% pdftk usually works fine. 
% 
% \textbf{Please do not use Apple's preview to cut off supplementary material.} In
% previous years it has altered margins, and created headaches at the camera-ready
% stage. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


